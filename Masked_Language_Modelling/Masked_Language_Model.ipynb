{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d82d7b1b-a2df-4242-8755-8c9e3ee6dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df353e71-2cf9-498f-a6c5-c2e0334cc3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c6ebea0d342cea61849dc6334146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prafu\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prafu\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ae8df5ff724922b2e4d067f398bc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf001cf89484b419d5fcc8698304390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64312979bb774caba2172c7db74d6319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d190cd29864d438ab67ccb22b8bbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eed107ba1e44396b7b30a89591a4d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afd81e4a2c6413caf4afda2ceba24e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bcc43dc-d8ea-4545-8700-2acfb1d26a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review : Directed by the duo Yudai Yamaguchi (Battlefield Baseball) and Jun'ichi Yamamoto \"Meatball Machine\" is apparently a remake of Yamamoto's 1999 movie with the same name. I doubt I'll ever get a chance to see the original so I'll just stick commenting on this one. First of what is \"Meatball Machine\" ? A simple in noway pretentious low budget industrial splatter flick packed with great make up effects and gore. It's not something you'll end up writing books about but it's nevertheless entertaining if you dig this type of cinema.<br /><br />\"Meatball Machine\" follows the well known plot. Boy loves girl but is too afraid to ask her on a date. Boy finally meets girl. Girl gets infected by a parasitic alien creature that turns her into a homicidal cyborg. Boy, in turn does also transform into said thing, and goes on a quest to save his love. Will he succeed? Who gives a damn, as long as there is carnage and death I'm satisfied.<br /><br />The plot is simple, relatively clich√©d but it does it's job well enough setting the movie's course straight forward into a bloody confrontation between the two leading characters. There is a subplot focusing on how the parasite that infected the girl came into to their lives. And yes it too luckily shows more violence. I'm happy. Acting is what you would expect from a no budget splatter film. It's not exactly painful for the ears but it's not exactly good either.<br /><br />The movie's main attraction besides the violence and gore (like I haven't mentioned that enough already) are the cyborg designs. Done by Keita Amemiya who's work in creating outlandish creatures and costumes for both movies and video-games is well known. The necroborgs as they are called in \"Meatball Machine\" look stunningly detailed. Without the usage of CGI Amemiya's designs are a breathtaking fusion of flesh and metal, painfully awesome in their appearance. Able to transforms various parts of the body into cool weaponry such as saws, rocket launchers, blood-firing shotguns and so on and so on. Though you can easily recognize the cheapness of the film, necroborgs are A-movie class.<br /><br />\"Meatball Machine\" is \"Tetsuo The Iron Man\" mixed up with \"Alien\" all done in low budget and extra ketchup mode. It's an immensely entertaining film that disregards modern special effects and proves that the splatter genre is still alive and kicking.\n",
      "Label : 1\n",
      "Review : I took part in a little mini production of this when I was a bout 8 at school and my mum bought the video for me. I've loved it ever since!! When I was younger, it was the songs and spectacular dance sequences that I enjoyed but since I've watched it when I got older, I appreciate more the fantastic acting and character portrayal. Oliver Reed and Ron Moody were brilliant. I can't imagine anyone else playing Bill Sykes or Fagin. Shani Wallis' Nancy if the best character for me. She put up with so much for those boys, I think she's such a strong character and her final scene when... Well, you know... Always makes me cry! Best musical in my opinion of all time. It's lasted all this time, it will live on for many more years to come! 11/10!!\n",
      "Label : 1\n",
      "Review : Like most comments I saw this film under the name of The Witching which is the reissue title. Apparently Necromancy which is the original is better but I doubt it.<br /><br />Most scenes of the witching still include most necromancy scenes and these are still bad. In many ways I think the added nudity of the witching at least added some entertainment value! But don't be fooled -there's only 3 scenes with nudity and it's of the people standing around variety. No diabolique rumpy pumpy involved!<br /><br />This movie is so inherently awful it's difficult to know what to criticise first. The dialogue is awful and straight out of the Troma locker. At least Troma is tongue in cheek though. This is straight-faced boredom personified. The acting is variable with Pamela Franklin (Flora the possessed kid in The Innocents would you believe!) the worst with her high-pitched screechy voice. Welles seems merely waiting for his pay cheque. The other female lead has a creepy face so I don't know why Pamela thought she could trust her in the film! And the doctor is pretty bad too. He also looks worringly like Gene Wilder.<br /><br />It is ineptly filmed with scenes changing for no reason and editing is choppy. This is because the witching is a copy and paste job and not a subtle one at that. Only the lighting is OK. The sound is also dreadful and it's difficult to hear with the appalling new soundtrack which never shuts up. The 'ghost' mother is also equally rubbish but the actress is so hilariously bad at acting that at least it provides some unintentional laughs.<br /><br />Really this film (the witching at least) is only for the unwary. It can't have many sane fans as it's pretty unwatchable and I actually found it mind-numbingly dull! <br /><br />The best bit was when the credits rolled - enough said so simply better to this poor excuse for a movie LIKE THE PLAGUE!\n",
      "Label : 0\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle().select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"Review : {row['text']}\")\n",
    "    print(f\"Label : {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa2e95-7bba-4579-972a-8b257f8ce188",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b50119-8744-4ca7-932f-9205b4714cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f9c7a3f484407ebbbc0e6d2f15b88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prafu\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prafu\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b6563fac4942cbb922d0f5a473e844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaa5162f422468893d40b308ff26a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55648d24fa024a79acfa2a83212c1c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1d8f2eb-ac74-40b3-a05f-6700e03c8360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572cfc3fa7c94d0ca92fa1ab631743b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6d85feac8b4d979e4738a24832afcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e46cdb69731426db60fe8cb39396916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        tokenized_inputs[\"word_ids\"] = [tokenized_inputs.word_ids(i) for i in range(len(tokenized_inputs[\"input_ids\"]))]\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a4024-be5d-49be-9bb1-a0ab6a69a602",
   "metadata": {},
   "source": [
    "There are sentences which are greater than the max length allowed in the model, so we will divide each sentence in chunks of fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e223487-4e10-4774-9c1d-6c05f5330c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35fce32-4ab0-4867-a4d7-d1fb65ec1c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 has length 363\n",
      "Review 1 has length 304\n",
      "Review 2 has length 133\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"Review {idx} has length {len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dfb1a40-d490-4ff4-8c08-b3f3beb6d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated length : 800\n"
     ]
    }
   ],
   "source": [
    "concatenated_samples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "print(f\"Concatenated length : {len(concatenated_samples['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2548a123-9f3a-47ca-afa3-45ed0b5f70e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length : 128\n",
      "Chunk length : 128\n",
      "Chunk length : 128\n",
      "Chunk length : 128\n",
      "Chunk length : 128\n",
      "Chunk length : 128\n",
      "Chunk length : 32\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i: i+CHUNK_SIZE] for i in range(0, len(concatenated_samples[k]), CHUNK_SIZE)]\n",
    "    for k, t in concatenated_samples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"Chunk length : {len(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff126e56-58a4-4061-bdf8-577cd18db030",
   "metadata": {},
   "source": [
    "The last chunk will be smaller than the max chunk size, so we can either drop it or pad it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82205e61-e2e2-4ea2-91ed-2712121bc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_text = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_text[list(examples.keys())[0]])\n",
    "    total_length = (total_length // CHUNK_SIZE) * CHUNK_SIZE\n",
    "\n",
    "    results = {\n",
    "        k: [t[i: i+CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)]\n",
    "        for k, t in concatenated_text.items()\n",
    "    }\n",
    "    results['labels'] = results['input_ids'].copy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a10a36a6-4161-4e48-8f14-ff612afea532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13732c721774238b43fda6a47dd463b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b930b406d11447d7bb784ab4aec28ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b32786d953640c68efdc235642e927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3373380-3878-4e3c-a8e3-5a06393bd4b3",
   "metadata": {},
   "source": [
    "### FineTuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30c1f77f-86a3-449c-9c41-4240b7876e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "790c4167-8b6f-4e54-8c34-f7a70d583328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS] i rented i am curious - yellow [MASK] my [MASK] store because of all the controversy that surrounded it when it [MASK] [MASK] released in 1967. i also heard that at first it was seized by u. s. customs [MASK] it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i originating had to see [MASK] [MASK] myself. < br / > < br / > the plot is centered around a keynote swedish drama [MASK] named lena who wants to learn everything she can about life. in particular she wants to [MASK] her attentions [MASK] making some sort of documentary [MASK] what the average swede thought about [MASK] [MASK] issues such\n",
      "\n",
      "as the [MASK] war [MASK] race issues in the united states. in between clubs politicians [MASK] ordinary denize [MASK] of stockholm about [MASK] opinions subtle politics, she has sex [MASK] her drama teacher, classmates, [MASK] married men. < br / > < [MASK] / > what sentence me [MASK] i am [MASK] - yellow is [MASK] 40 years ago, this was considered pornographic. really [MASK] the sex and nudity scenes are few and far between [MASK] [MASK] then it ' [MASK] not shot [MASK] some cheaply made [MASK]o. while my countrymen mind find it [MASK], in reality sex and nudity are a [MASK] staple in [MASK] cinema. even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n{tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc7fadc6-0985-4551-b3bd-673a0b006b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_datasets = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size = train_size, test_size = test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2470d860-b4cc-4e59-b94b-a8121c11ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(downsampled_datasets[\"train\"]) // batch_size\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"distilbert-finetuned-mlm-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=downsampled_datasets[\"train\"],\n",
    "    eval_dataset=downsampled_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7f26751-fc55-4866-9f5e-a07ba66012cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 04:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 21.94\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6d4ef-a619-4e39-b503-189969f3a78c",
   "metadata": {},
   "source": [
    "Lower perplexity means a better language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0adbad7a-2e90-4801-9044-168e8f9fa353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 10:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.680400</td>\n",
       "      <td>2.493174</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.583200</td>\n",
       "      <td>2.448004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.525500</td>\n",
       "      <td>2.480797</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=2.595887607323389, metrics={'train_runtime': 656.9512, 'train_samples_per_second': 45.665, 'train_steps_per_second': 0.717, 'total_flos': 994208670720000.0, 'train_loss': 2.595887607323389, 'epoch': 3.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a778baca-14f0-4b8c-8bda-9820e32172f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 12.02\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12095b5e-a123-4d0b-a56a-93f7e3c7cd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace84dce6a3d4186982d5fe379dd8e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/praful-goel/distilbert-finetuned-mlm-imdb/commit/b0d4bd2804f393f6453635c5c1b4259a43f0746d', commit_message='End of training', commit_description='', oid='b0d4bd2804f393f6453635c5c1b4259a43f0746d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/praful-goel/distilbert-finetuned-mlm-imdb', endpoint='https://huggingface.co', repo_type='model', repo_id='praful-goel/distilbert-finetuned-mlm-imdb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b3e0b-d351-4c57-99bd-9f06c0d5b4de",
   "metadata": {},
   "source": [
    "### Using our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "930f06c1-ada2-4148-976f-71ca6a9fea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "736752fe-0c56-4911-ba20-bf0aae2779de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e55737f07f4d569c968096db66583e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prafu\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prafu\\.cache\\huggingface\\hub\\models--praful-goel--distilbert-finetuned-mlm-imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc1e26a7b94c6eb9730f40cb77971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1556741e852c433b9e66ec646fef7487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd431908883547659ea1e16f788f6ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8396dc27e9184e8bb05f1bc4a799c1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149d251b1ef54d5d8b4c511fecfe0933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great!\n",
      ">>> this is a great.\n",
      ">>> this is a great deal\n",
      ">>> this is a great film\n",
      ">>> this is a great movie\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"praful-goel/distilbert-finetuned-mlm-imdb\"\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=model_checkpoint)\n",
    "\n",
    "text = \"This is a great [MASK]\"\n",
    "\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969f731-2a18-4700-a8d8-b6d3fe5e4944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
