{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f77fac-0c4c-4fe2-9804-400982d9a375",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49d52e41-0e83-4d33-aa85-b22ffe120eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283c4ba6-6e23-4be4-8134-1af552cfb788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051c6b55b2e4526b76af9296327beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3705024c76ab4c58ab1627df70bc488e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515c40f1b38476dbf7f823757a4dafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df21d8cdbb794616955387234004a405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cdfe68-65a3-4893-a9e1-4fdf61871f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561809cc-aedb-409f-849b-6fe3d8b51831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc2c5ac-6309-42ca-8383-57cb54f62daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a01ff62-c215-4717-acac-62f4d302eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU, label : B-ORG\n",
      "rejects, label : O\n",
      "German, label : B-MISC\n",
      "call, label : O\n",
      "to, label : O\n",
      "boycott, label : O\n",
      "British, label : B-MISC\n",
      "lamb, label : O\n",
      "., label : O\n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    print(f\"{word}, label : {full_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c0c69-8abc-4351-b269-24765c04ce85",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3828a83-1029-4392-833d-09fdf4fad36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd497086-ce9d-44fe-99c3-65f415d0e7d3",
   "metadata": {},
   "source": [
    "To tokenize a pre-tokenized sentence, we just need to pass is_split_into_words=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037bcc2e-92e0-4c17-9b2e-39ead4ed9231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(sample_inputs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a1826-2c43-4baf-ae22-2c2508bf532c",
   "metadata": {},
   "source": [
    "Here, as we can see the word lamb is split into two, creating one extra token. i.e 10 but originally we have only 9 labels for this input, so we need to align the labels with the tokens. For token inside the word, we replace B- with I-. Also for the special tokens, the label will be default to -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8698eedd-f0d9-4e41-bfda-683ee1bd7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    curr_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != curr_word:\n",
    "            curr_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            # Replace B- with I-\n",
    "            if label%2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0222a621-5b92-4416-a3b1-01441f63ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = sample_inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06815a89-6b2e-42b0-919b-7c3ced574443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    \n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285b1540-165e-4980-9712-df284b27c2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290eaceeb0344956932a27b371d0ad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8a51b7ca2496b9cb65440745343ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0841cde6fb4206a67706aaec5b483e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46fdea80-cf83-47bd-af1c-e0a1b7716574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  7270,\n",
       "  22961,\n",
       "  1528,\n",
       "  1840,\n",
       "  1106,\n",
       "  21423,\n",
       "  1418,\n",
       "  2495,\n",
       "  12913,\n",
       "  119,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95034ed8-fee6-4048-96d3-98f4460851bf",
   "metadata": {},
   "source": [
    "### FineTuning the model with Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "255e3730-77df-4427-bd8e-830f7cbd7559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "# Here if we try on a sample of 2 inputs, we can see that it also padded labels to -100 to denote unnecessary labels\n",
    "sample_batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "sample_batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e477fce8-1b28-4b30-b776-e6dbdc1c17b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f35783408e14ee2854c93fda4dd5b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "befb9113-e04e-4e33-9943-1bb16660c0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test this metric\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e611a64-d62a-47a3-8693-f7a8b706822d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "# introducing an error to test\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8135cc48-af48-4c6c-8951-8ffd803b37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"Overall Precision\": all_metrics[\"overall_precision\"],\n",
    "        \"Overall Recall\": all_metrics[\"overall_recall\"],\n",
    "        \"Overall F1\": all_metrics[\"overall_f1\"],\n",
    "        \"Overall Accuracy\": all_metrics[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9773d17-7217-45d7-a971-7e5c0719f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a9b8ed0-7936-4f72-a9d4-c0628b5c5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4579956-19c7-4f76-945a-62882a2933af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "998c90d9-a189-4d7a-be02-81e215aa4787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10351b7a690a453197c63b5b75087e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b0e711f-8c20-487e-a54c-9a29f9c07275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 17:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Overall precision</th>\n",
       "      <th>Overall recall</th>\n",
       "      <th>Overall f1</th>\n",
       "      <th>Overall accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.060585</td>\n",
       "      <td>0.909521</td>\n",
       "      <td>0.937227</td>\n",
       "      <td>0.923166</td>\n",
       "      <td>0.982781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.065222</td>\n",
       "      <td>0.933245</td>\n",
       "      <td>0.948166</td>\n",
       "      <td>0.940646</td>\n",
       "      <td>0.986019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.061365</td>\n",
       "      <td>0.938216</td>\n",
       "      <td>0.950690</td>\n",
       "      <td>0.944412</td>\n",
       "      <td>0.987019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06008250593867552, metrics={'train_runtime': 1028.174, 'train_samples_per_second': 40.969, 'train_steps_per_second': 5.124, 'total_flos': 920771584279074.0, 'train_loss': 0.06008250593867552, 'epoch': 3.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbcb49a0-9d4e-449d-b0fb-53ca73269b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecfea7c0a984c3ba349a0886964e40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/praful-goel/bert-finetuned-ner/commit/1e4e3e8818561c4d8700177e69f1a8daf90b7a72', commit_message='Training complete', commit_description='', oid='1e4e3e8818561c4d8700177e69f1a8daf90b7a72', pr_url=None, repo_url=RepoUrl('https://huggingface.co/praful-goel/bert-finetuned-ner', endpoint='https://huggingface.co', repo_type='model', repo_id='praful-goel/bert-finetuned-ner'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f9c52-cca2-40e2-ba4f-3f8bdef2403e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab1ff26a-b51c-43f0-99fe-d276a26921d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "debb0466-9db3-48f1-8f6d-b5ee1ddd1f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75514138b39d4d8b89fb11d787a28644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prafu\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prafu\\.cache\\huggingface\\hub\\models--praful-goel--bert-finetuned-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24190234a1714f58918dd6453d4ece7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc21f0cebb4447c7b8a60801e05dfdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16837a509d384197b96d85460b882405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e3c365139043f3963f8a6bd769b064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91454c9616b5463a99c848c713ed6260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99808043,\n",
       "  'word': 'Praful Prakash Goel',\n",
       "  'start': 11,\n",
       "  'end': 30},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6514068,\n",
       "  'word': 'IIIT',\n",
       "  'start': 46,\n",
       "  'end': 50},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9334156,\n",
       "  'word': 'Guwahati',\n",
       "  'start': 51,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"praful-goel/bert-finetuned-ner\"\n",
    "finetuned_pipeline = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "finetuned_pipeline(\"My name is Praful Prakash Goel and I study in IIIT Guwahati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c603859-e66b-4f81-bc27-b27d19aa2836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
